# -*- coding: utf-8 -*-
"""CDL_EjercicioAPIClima.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DUmGUdjaZ4_QNoYihEfWi5chv7Kj7dzK

# Ejercicio para desarrollo de modelo de aprendizaje automático

## **Requisitos:**

- Obtener de la API de Open-Meteor la información de parámetros diarios: `"daily": ["temperature_2m_max", "temperature_2m_min", "temperature_2m_mean", "rain_sum", "precipitation_hours", "wind_speed_10m_max", "shortwave_radiation_sum"]`
- Escoger un rango de fechas (de preferencia, dos años o más)
- Revisar que no existan campos duplicados, nulos.
- Escoger variable objetivo (lluvia, radiación, viento o temperatura)
- Hacer un análisis exploratorio (histogramas, gráficos de caja, matriz de correlación)
- Escalado de datos (MinMax, Standard)
- Implementar un algoritmo de aprendizaje (RandomForest u otro)
- Imprimir métricas resultantes de desempeño

### Importación de librerias
"""

import requests  # Para realizar solicitudes HTTP
import pandas as pd  # Para manipulación de datos
import numpy as np  # Para operaciones numéricas
import matplotlib.pyplot as plt  # Para gráficos
import seaborn as sns  # Para visualización de datos
from sklearn.model_selection import train_test_split  # Para dividir datos en entrenamiento y prueba
from sklearn.preprocessing import MinMaxScaler, StandardScaler  # Para escalar datos
from sklearn.ensemble import RandomForestRegressor  # Modelo de aprendizaje automático
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Métricas de evaluación
# Entrenar el modelo RandomForest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

"""### Extracción de datos de GitHub"""

url = "https://raw.githubusercontent.com/CirceDelLop/ClimaAcademiaDataEngineer/refs/heads/main/ejercicio/archivoscsv/run-1739065936191-part-r-00000.csv"
df_clima = pd.read_csv(url)
df_clima.head()

"""### Renombrar columnas"""

df_clima.rename(columns={
    "time": "fecha",
    "temperature_2m_max": "temperaturaMax",
    "temperature_2m_min": "temperaturaMin",
    "temperature_2m_mean":"temperaturaMedia",
    "rain_sum":"lluviaTotal",
    "precipitation_hours": "horasPrecipitacion",
    "wind_speed_10m_max": "velocidadViento",
    "shortwave_radiation_sum": "totalRadiacion"
}, inplace=True)
df_clima.head()

"""### Análisis exploratorio"""

# Información del conjunto de datos
df_clima.info()

# Lista de columnas numéricas
numerical_columns = ["temperaturaMax", "temperaturaMin", "lluviaTotal", "horasPrecipitacion", "velocidadViento","totalRadiacion"]

# Histograma
plt.figure(figsize=(15, 10))

for i, col in enumerate(numerical_columns):
    plt.subplot(2, 3, i+1)
    sns.histplot(df_clima[col].dropna(), kde=True, bins=30, color="skyblue")
    plt.title(f'Distribución de {col}')

plt.tight_layout()
plt.show()

# Seleccionar solo las columnas numéricas
df_numericas = df_clima.select_dtypes(include=["number"]).drop(columns=["year", "month", "day"], errors="ignore")

# Matriz de correlación
plt.figure(figsize=(10,10))
sns.heatmap(df_numericas.corr(), annot=True, fmt='.2f')
plt.show()

"""### Escalado de datos (MinMax, Standard)"""

# Lista de columnas numéricas
numerical_columns = ["temperaturaMax", "temperaturaMin", "lluviaTotal", "horasPrecipitacion", "velocidadViento","totalRadiacion"]

# Copia del dataset para la prueba de StandarScaler()
df_clima_ss = df_clima.copy()

# Implementación de StandarScaler sobre los datos numéricos
scaler_standar = StandardScaler()
df_clima_ss[numerical_columns] = scaler_standar.fit_transform(df_clima_ss[numerical_columns])

df_clima_ss.describe()

#Crear una copia del DataFrame para escalado
df_clima_escalado=df_clima.copy()

# Aplicar el escalado a las columnas numéricas
# MinMaxScaler hace un escalado de valores por defecto entre 0 y 1.
scaler = MinMaxScaler()
df_clima_escalado[numerical_columns] = scaler.fit_transform(df_clima_escalado[numerical_columns])

#Mostrar una vista previa del DataFrame escalado
print("Primeras filas del DataFrame escalado:")
df_clima_escalado.describe()

"""### algoritmo de aprendizaje (RandomForest)"""

# Lista de columnas numéricas
numerical_columns = ["temperaturaMax", "temperaturaMin","temperaturaMedia"]

df_clima_escalado = df_clima.copy()

# MinMaxScaler hace un escalado de valores por defecto entre 0 y 1.
scaler = MinMaxScaler()
df_clima_escalado[numerical_columns] = scaler.fit_transform(df_clima_escalado[numerical_columns])

# Mostrar una vista previa del DataFrame escalado
print("Primeras filas del DataFrame escalado:")
df_clima_escalado.describe()

threshold = df_clima_escalado['temperaturaMax'].median()
df_clima_escalado['temperaturaAlta'] = (df_clima_escalado['temperaturaMax'] > 0.555556).astype(int)

# Definir las variables predictoras y la variable objetivo (variable a predecir)
predictoras = ["temperaturaMax", "temperaturaMin"]
objetivo = "temperaturaMedia"

# Variables predictoras (X) y variable objetivo (y)
X = df_clima_escalado[predictoras]
y = df_clima_escalado[objetivo]

# Dividir los datos en conjuntos de entrenamiento y prueba (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Categorizar valores de y_train
threshold = 50  # Umbral
y_train = (y_train > threshold).astype(int) # y_train>50=>y_train=1; _train<50=>y_train=0

# Convertir las predicciones continuas a clases discretas (por ejemplo, 3 clases)
bins = [-np.inf, 0.5, 1.5, np.inf]  # Definir los límites de las clases
labels = [0, 1, 2]  # Asignar clases discretas
y_pred_discretized = np.digitize(y_pred_rf, bins) - 1  # Convertir las predicciones continuas en categorías

# Entrenar el modelo RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Realizar predicciones y evaluar el modelo RandomForest
y_pred_rf = rf_model.predict(X_test)

# Calcular las métricas de clasificación
accuracy = accuracy_score(y_test_discretized, y_pred_discretized)
precision = precision_score(y_test_discretized, y_pred_discretized, average='macro', zero_division=True)
recall = recall_score(y_test_discretized, y_pred_discretized, average='macro', zero_division=True)
f1 = f1_score(y_test_discretized, y_pred_discretized, average='macro', zero_division=True)

print("Accuracy:", accuracy)
print("Precision (macro):", precision)
print("Recall (macro):", recall)
print("F1-score (macro):", f1)